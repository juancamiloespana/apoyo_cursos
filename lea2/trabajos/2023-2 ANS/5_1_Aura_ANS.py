# -*- coding: utf-8 -*-
"""MorenoDíazAuraLuz - NoSupervisadoipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yXOLDua4EpZQefxaK3t4OkFo6K_yu-U0

'''

FACULTAD DE INGENIERÍA<br>
DEPARTAMENTO DE INGENIERÍA INDUSTRIAL<br>
ANALITICA PARA LA TOMA DE DECISIONES<br>
TRABAJO DEL CURSO - PRIMERA ENTREGA: NO SUPERVISADO <br>
Semestre 2023-02<br>

Por: : Aura Luz Moreno Díaz

---

La base de datos que utilizaremos para el trabajo mañana se encuentra en el github del curso en la carpeta _data. Se llama credit_card_clients.csv, la descripción de los campos está en credit_card_clientes_dictionary.txt

La idea es realizar reducción de dimensiones y una segmentación y responder unas preguntas de análisis con base en los resultados.

###DICCIONARIO DE DATOS:
CUST_ID: Identificación del titular de la tarjeta de crédito<br>

BALANCE: Saldo disponible para compras<br>

BALANCE_FREQUENCY: Frecuencia de actualización del saldo (donde 1 = actualización frecuente y 0 = no se actualiza con frecuencia).<br>

PURCHASES: Importe de las compras realizadas por el cliente<br>

ONEOFF_PURCHASES: Importe máximo de la compra realizada a una couta<br>

INSTALLMENTS_PURCHASES: Importe de las compras realizadas a plazos<br>

CASH_ADVANCE: Avances de Efectivo realizados por el usuario<br>

PURCHASES_FREQUENCY: Frecuencia con la que se realizan las compras (1 = compras frecuentes, 0 = compras poco frecuentes)<br>

ONEOFF_PURCHASES_FREQUENCY: frecuencia con la que se realizan las compras a una couta (1 = compras frecuentes, 0 = compras poco frecuentes)<br>

PURCHASES_INSTALLMENTS_FREQUENCY: Frecuencia con la que se realizan compras a plazos (1 = se realizan con frecuencia, 0 = no se realizan con frecuencia)<br>

CASH_ADVANCE_FREQUENCY: Frecuencia con la que se realizan avances de efectivo <br>

CASH_ADVANCE_TRX: Número de transacciones realizadas por "avances de efectivo"<br>

PURCHASES_TRX: Número de transacciones de compra realizadas<br>

CREDIT_LIMIT: Límite de la tarjeta de crédito del usuario<br>

PAYMENTS: Importe de los pagos realizados por el usuario<br>

MINIMUM_PAYMENTS: Importe mínimo de los pagos realizados por el usuario<br>

PRC_FULL_PAYMENT: Porcentaje del pago total abonado por el usuario<br>

TENURE: Tenencia del servicio de tarjeta de crédito para el usuario<br>
"""

!pip install kneed
!pip install clusteval
!pip install factor_analyzer

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import cluster
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler

# Para regla del codo
from kneed import KneeLocator

# Para PCA
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from factor_analyzer import FactorAnalyzer

from clusteval import clusteval ### para detecter numero de cluster automáticamente

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ANALITICA 2/NO SUPERVISADO/TRABAJO/credit_card_clients.csv')

df.head()

df.value_counts

df.info()

"""##PRUEBA DE KMEANS"""

#Quiero hacer la prueba comparando Kmeans

features=df[['BALANCE','PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PAYMENTS']] #Son las columnas que vamos a utilizar para hacer las pruebas

k=4

kmedias=cluster.KMeans(n_clusters=k) ## crea el modelo
kmedias.fit(features) ## ajusta modelo a datos. COn esto queda el modelo entrenado

#Se genera la columna con el grupo y agregarla a la base de datos original
cluster_label= kmedias.labels_ ##numeros de los cluster
cluster_label

df['cluster']= cluster_label ## agregar clusters a dataframe

centroides=kmedias.cluster_centers_ ## centroides
centroides

### grafica de cluster y centroides
sns.scatterplot(x='BALANCE', y ='PURCHASES', hue='cluster',data=df, palette="viridis")
plt.scatter(x=centroides[:,0], y = centroides[:,1], marker='o', s=200)
plt.show()

### grafica de cluster y centroides
sns.scatterplot(x='INSTALLMENTS_PURCHASES', y ='CASH_ADVANCE', hue='cluster',data=df, palette="viridis")
plt.scatter(x=centroides[:,0], y = centroides[:,1], marker='o', s=200)
plt.show()

x=df[['BALANCE','PURCHASES']]

x_s=StandardScaler().fit_transform(x) ### se estandarizan las columnas.

###identificar el mejor k - método del codo y el de silhouette

wcss=[] #METODO DEL CODO
sil=[]  #METODO DE SILUETA (Silhouette)

for k in range(1,12):
    km=cluster.KMeans(n_clusters=k, n_init=10)  #Km es el modelo para kmedias
    km.fit(x_s) #Ajustarlo con los datos escalados
    wcss.append(km.inertia_) ##inertia = wcsss, la suma de cuadrados dentro del cluster (promedio de todos los clusters)
    label=km.labels_
    if k>1: #Silouette debe empezar despues de uno, es decir, a partir de 2
        sil_avg=silhouette_score(x_s,label ) #Silhouette_score está dentro de SKLearn y dentro de metrics
        sil.append(sil_avg)

sns.lineplot(x=np.arange(1,12), y=wcss, marker="o", palette="viridis")
sns.lineplot(x=np.arange(2,12), y=sil, marker="o", palette="viridis")

kl=KneeLocator(x=np.arange(1,12), y=wcss,curve="convex", direction="decreasing")
kl.elbow

cl=clusteval(cluster="kmeans", evaluate="silhouette")
cl.fit(x_s)
cl.plot()

"""#REDUCCION DE DIMENSIONALIDAD

##PCA
"""

df2 = df.iloc[:, 1:]  #Quito la primera columna de CUST_ID ya que es irrelevante la identificación del cliente
df2.head()

"""Al analizar las columnas de la base de datos y su contenido, se puede observar que tenemos variables porcentuales, otras de frecuencias y otras numéricas.
Por ejemplo, Balance nos indica el saldo total de la cuenta, la cual se podría escalar con las compras realizadas a una cuota, los pagos minimos y los totales<br>
Por otro lado también se podrian analizar las frecuencias y luego ver como están correlacionadas.
"""

df2 = df2.dropna()

feat_sc=StandardScaler().fit_transform(df2) #Procedo a hacer un escalado de variables
feat_sc

pca=PCA(n_components=4) #el numero de componentes es el numero de variables originales
prueba= pca.fit(feat_sc) #ajuste de los datos
prueba

"""Esto quiere decir que tenemos 4 componentes principales que capturan la mayoría de la variabilidad. PERO: Con tanta información, solo estos 4 componentes me darán la información necesaria sin sacrificar confiabilidad?"""

################
pca = PCA(n_components=4)  # Elige el número deseado de componentes principales
principal_components = pca.fit_transform(feat_sc)

############################
pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4'])
pca_df

"""PC1 - Podria estar relacionado con la actividad general de los clientes en el uso de las TC con respecto a los gastos que realiza <br>
PC2 - Podria estar relacionado con las frecuencias y los comportamientos de compras<br>
PC3 - Pueden ser los patrones de pago y su cumpliento
PC4 - Los comportamientos menos comunes ya que se notan valores negativos
"""

pca.components_ ## lambdas, vectores propios pesos de observadas sobre latentes
#las filas son las variables de salida
#Las columnas son el numero de filas de las variables originales#

pca.explained_variance_ ## valores propios alpha, cuánta varianza es explicada
#Es uno por cada componente generado, en este caso tenemos 4

ve=pca.explained_variance_ratio_ ### procentaje de variable explicada por cada componente
ve #VE es la Varianza Explicada
#En este caso el primer componente explica el 26%, el segundo el 21%, el tercero el 8% y el ultimo el 7%

"""En este caso, la varianza explicada del primer componente es del 26% y el segundo explica el 21% (aproximadamente), esto nos da casi un 48%. Esto responde la segunda pregunta-<br><br>
0.26281396 + 0.21630588 + 0.08433123 + 0.07153161 ≈ 0.63598268<br>

Se utilizan cuatro variables latentes (componentes principales) para representar los datos, y se puede explicar alrededor del 63.6 de la variación total en el comportamiento de los clientes de tarjetas de crédito mediante estas cuatro variables latentes.
"""

l = pca.transform(feat_sc) ## variables latentes
l[0] ## variables latentes para primera fila

l_manual=np.dot(feat_sc,pca.components_.T ,) ## calcula los l manualmente con producto punto
l_manual[0]

"""Estas coordnadas manuales coinciden con las anteriores, por ende vamos en dirección correcta."""

sns.lineplot(x=np.arange(1,5), y=np.cumsum(ve), palette="viridis")

#Con esta grafica podemos explicar la regla del codo
#ya se puede usar el Kmeans aqui

l_sel=l[:,0:2]
l_sel #Son los dos componentes seleccionados de cada fila

####3 para graficar los datos originales con la transformación

sns.scatterplot(x=l_sel[:,0], y=l_sel[:,1], hue=df2['PURCHASES'], palette="viridis")

"""##FACTOR ANALYSIS - DESCARTADO

Esta prueba la hago para revisar si los gráficos quedan con similitudes si se toman las mismas variables y a PURCHASES como eje
"""

fa=FactorAnalyzer(n_factors=4, rotation=None)
fa.fit(feat_sc) #Se hace el ajuste de los datos

w=fa.loadings_ ## WWpeso de variables latentes sobre las x - Cargas factoriales
w

"""Estas cargas factoriales deberian estar entre -1 y 1, por ende son valores aceptables.

En este caso se tienen 4 variables latentes
"""

l=fa.transform(feat_sc) ### variables latentes
l[0]

lamb=fa.weights_ ## peso de variables observadas sobre las latentes
lamb

ve=fa.get_factor_variance() ## varianza explicada, porcentaje de varianza explicada, porcentaje de varianza explicado acumulado
ve

sns.scatterplot(x=l[:,0], y=l[:,1], hue=df2['PURCHASES'], palette="viridis")

"""En este caso, creo que PCA explica mejor el agrupamiento de los datos

#ESTRATEGIAS

En el algoritmo de Kmeans usé: BALANCE, PURCHASES, INSTALLMENTS_PURCHASES, CASH_ADVANCE y PAYMENTS para hacer un acercamiento.<br>

El grupo que tienen un saldo bajo, en un nivel moderado de compras (PURCHASES), un bajo nivel de compras a plazos (INSTALLMENTS_PURCHASES), un bajo uso de avances de efectivo (CASH_ADVANCE) y un alto nivel de pagos (PAYMENTS). Con esto podríamos saber que tenemos un grupo de clientes que paga cumplidamente y que tiene un comportamiento óptimo, a ellos se les podrían ofrecer ofertas más atractivas

Para los quetienen un saldo moderado o intermedio:  realiza muchas compras (PURCHASES), compra a plazos (INSTALLMENTS_PURCHASES) con frecuencia, utiliza menos avances de efectivo (CASH_ADVANCE) y realiza pagos sustanciales (PAYMENTS).Estos clientes que pagan a tiempo pero que usan con mucha frecuencia las tarjetas de crédito son muy redituables pero también riesgosos, sin embargo se pueden crear estrategias par que continuen su hábito de pagos.

Para los que tienen un saldo muy alto, es decir los clientes de alto riesgo de quedar en mora, realiza compras considerables (PURCHASES), compra a plazos (INSTALLMENTS_PURCHASES) con frecuencia, utiliza avances de efectivo (CASH_ADVANCE) y realiza pagos significativos (PAYMENTS). Estrategias que eviten los avances en efectivo y que incentiven el pago a tiempo.

# PREGUNTA 1
1. Cuáles son las variables observadas más importantes en las dos variables latentes más importantes.<br>
Estas deben revisarse en el análisis factorial, que podrian ser PURCHASES" e "INSTALLMENTS_PURCHASES"

# PREGUNTA 2
2. Qué porcentaje de variación va a explicar y cuántas variables latentes va a utilizar para representar los datos.<BR>
para esto usé: ve = fa.get_factor_variance() que está en el FactorAnalysis modulo [37]<br>
(array([4.32109528, 3.17103603, 1.23966156, 0.83417397]),<br>
 array([0.25418208, 0.18653153, 0.07292127, 0.04906906]),<br>
 array([0.25418208, 0.44071361, 0.51363487, 0.56270393]))<br><br>

 Varianza total: 4.32<br>
 Porcentaje de varianza explicada: 0.254<br>
 Porcentaje de varianza explicada acumulada: 0.44

#PREGUNTA 3
3. Diga cuáles son las principales diferencias entre los segmentos encontrados.
<BR>
lamb = fa.weights_
<BR>
PC1 - Podria estar relacionado con la actividad general de los clientes en el uso de las TC con respecto a los gastos que realiza <br>
PC2 - Podria estar relacionado con las frecuencias y los comportamientos de compras<br>
PC3 - Pueden ser los patrones de pago y su cumpliento
PC4 - Los comportamientos menos comunes ya que se notan valores negativos
"""