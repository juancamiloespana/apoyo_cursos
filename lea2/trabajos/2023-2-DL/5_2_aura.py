# -*- coding: utf-8 -*-
"""TrabajoFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkpiD43XwnxuAJKat9I7Qy31QevyOAXz

#TRABAJO FINAL ANALITICA <br>
##Por: AURA LUZ MORENO DÍAZ

# Datos
Los datos que se utilizará contiene el comportamiento de uso de aproximadamente 9.000 usuarios de tarjetas de crédito en dos periodos de tiempos diferentes.
descripción campos: credit_card_clientes_dictionary.txt
carpeta: https://github.com/juancamiloespana/LEA2/tree/master/_data

Dataset 1: credit_card_clients.csv

La variable ‘PURCHASES’ fue calculada de Ene-Jun de 2023
El resto de las variables fueron calculadas seis meses antes Jul-Dic 2022

Dataset 2: credit_card_clients2.csv

Todas las variables fueron calculadas en el periodo Ene-Jun 2023

Trabajo a desarrollar:

La empresa quiere predecir cuántos será el monto de compras de cada cliente “PURCHASES” en el periodo de Jul-Dic 20223. Esta variable es fundamental para la empresa porque las compras del cliente son proporcionales a los ingresos que genera a la empresa. Con estas predicciones se quieren definir estrategias de fidelización, en el caso de los clientes que mayores compras vayan a tener, y estrategias para mejorar el uso de las tarjetas en los clientes que van a tener menor cantidad de compras.

Dentro de sus análisis tenga en cuenta analizar (como mínimo) los siguientes puntos:

1. Utilizando redes neuronales, entrene el mejor modelo posible que le permita predecir las compras “PURCHASES” que va a tener un cliente en los próximos 6 meses, utilizando la información de 6 meses atrás.
2. Analice el desempeño del modelo entrenado y saque algunas conclusiones.
3. Realice la predicción de las compras “PURCHASES” de los clientes en el periodo Jul-Dic 2023 e identifique el top 10 de clientes que más compras van a tener y top 10 de los clientes que menos compras van a tener.
4. Defina que estrategias implementaría para fidelizar clientes con muchas compras y qué estrategias implementaría para activar clientes que están usando muy poco las tarjetas.

Seleccionamos las librerias que utilizaremos a lo largo del trabajo:
"""

!pip install keras_tuner

#paquetes básicos
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#importar paquetes de sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error


#importar paquetes de redes neuronales

import tensorflow as tf
from tensorflow import keras

#Paquete para afinamiento nn de tensorflow
import keras_tuner as kt

#Paquetes de evaluación de modelos sklearn

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn import metrics

"""Hacemos lectura de los datos desde github"""

url="https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients.csv"
credit_card_data = pd.read_csv(url)

"""Determinamos la proporción de nulos de la base de datos:"""

porcentaje_nulos = credit_card_data.isnull().mean() * 100

porcentaje_nulos

"""Eliminamos los datos nulos ya que no son relevantes, son solo el 3.4% de la base de datos:"""

credit_card_data.dropna(inplace=True)

"""Volvemos a revisar los nulos:"""

credit_card_data.info()

"""Ahora tomaremos una muestra aleatoria de 200 datos y lo guardaremos en un subset de datos para hacer pruebas posteriores."""

credit_card_data_subset = credit_card_data.sample(n=200)
credit_card_data_subset.to_csv('credit_card_data_subset.csv', index=False)
credit_card_data_subset.info()

"""Hacemos una separación de los datos para poder predecir la variable "purchases" la cual es nuestra variable a objetivo."""

y_credit = credit_card_data['PURCHASES']
X_credit = credit_card_data.drop('PURCHASES', axis=1)

"""Seleccionamos solamente las columnas numéricas del set de datos X_credit y lo reasignamos para tomar solamente estas columnas."""

numeric_columns = X_credit.select_dtypes(include=[np.number]).columns
X_credit = X_credit[numeric_columns]

"""Hacemos los ajustes para el escalado:"""

sc=StandardScaler().fit(X_credit)

"""Y rellenamos con la media para aquellos con datos atipicos"""

y_credit = y_credit.fillna(y_credit.mean())

"""Procedemos ahora al escalado de datos, con fit calculamos la media y la desviación estándar para poder estandarizar las características. Luego usamos las medias de las desviaciones estándar en donde se guardan en un set de datos que tienen una media de cero y una ds de uno."""

scaler = StandardScaler().fit(X_credit)
X_credit_scaled = scaler.transform(X_credit)

"""Ahora traemos la biblioteca Joblib para poder guardar el conjunto de datos escalado para que pueda ser reutilizado sin tener que escalarlo nuevamente. Se guardará en sc.joblib"""

#Exportar el escalador
import joblib

joblib.dump(scaler, "/content/drive/MyDrive/Colab Notebooks/ANALITICA 2/TFINAL/sc.joblib")

"""Ahora dividimos los datos en datos de entrenamiento y prueba.  El 20% de los datos se utilizarán como conjunto de prueba. El 80% se usará como conjunto de entrenamiento. Tenemos una semilla de **42** para la generación de números aleatorios, lo que asegura que la división sea reproducible. <br>

Se guardan  los conjuntos resultantes en cuatro variables distintas:

X_credit_train: Conjunto de entrenamiento de características.<br>
X_credit_test: Conjunto de prueba de características.<br>
y_credit_train: Conjunto de entrenamiento de la variable objetivo.<br>
y_credit_test: Conjunto de prueba de la variable objetivo.
"""

# Dividir los datos en conjuntos de entrenamiento y prueba
X_credit_train, X_credit_test, y_credit_train, y_credit_test = train_test_split(
    X_credit_scaled, y_credit, test_size=0.2, random_state=42
)

"""Las redes neuronales están compuestas de varias capas de neuronas. En la capa cero generalmente tenemos los parámetros de entrada y el escalado de las variables<br>
En las capas intermedias u ocultas tenemos funciones de activación para cada una. Los **interceptos** o **BIAS** se definen para cada neurona.

En este caso tenemos un problema de **REGRESIÓN** porque estamos tratando de predecir un valor (que es mayor a cero) donde se predicen las **compras** o PURCHASES

Ahora procedemos a crear la red neuronal para el problema de regresión de las compras con tarjeta de crédito.<br>
Definimos uno de los hiperparámetros llamado 'DOR' (Dropout Rate) que varía entre 0.1 y 0.6 con un paso de 0.1, para la tasa de pérdida.<br><br>
Otro de los hiperparámetros es 'opt' que toma valores de la lista ('adam', 'sgd'). Este valor se utiliza para seleccionar el optimizador entre Adam y SGD.
"""

# Definir la arquitectura del modelo de red neuronal
def build_credit_model(hp):
    dor = hp.Float('DOR', min_value=0.1, max_value=0.6, step=0.1)
    opt = hp.Choice('opt', ['adam', 'sgd'])

    model = keras.models.Sequential([
        keras.layers.Input(shape=(X_credit.shape[1],)),
        keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),
        keras.layers.Dropout(dor),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(32, activation="relu"),
        keras.layers.Dense(1, activation="linear")  # Cambiado a 'linear' para problemas de regresión
    ])

    if opt == 'adam':
        optimizer = keras.optimizers.Adam(learning_rate=0.001)
    else:
        optimizer = keras.optimizers.SGD(learning_rate=0.001)

    model.compile(optimizer=optimizer, loss = keras.losses.MeanAbsoluteError(), metrics=[keras.metrics.MeanAbsolutePercentageError()])
    return model

"""Aquí estableceremos los parámetros, es decir:


*   Weights o pesos
*   BIAS o Interceptos

Además de esto, Definimos los hiperparámetros de optimización: función de pérdida y la métrica.

También tenemos un learning rate que ajustamos en 0.01 como es la recomendación general.

Y los EPOCH que deben estar entre 5 y 10, para este caso elegimos 10

Entre las métricas tenemos para regresión:



*   RMSE
*   MAPE (El Porcentual es más facil de interpretar. Cuando la variable respuesta toma valores de cero, eso se daña. Mean Absolute Porcentage error es la más fácil de interpretar, te dice en porcentaje cuanto se esta desviando y te da intituivamente si esta bien o no. Es el opuesto del accuracy que si se acerca al 100, es excelente)

*   MAE (Esta en las mismas unidades de la variable respuesta y es más facil de interpretar. Cuantas unidades está variando el modelo de la predicción)


<br>
En este caso, se usa MSE o error cuadrático medio que es muy eficiente para problemas de REGRESIÓN como es el caso de nosotros.

<hr>
Ahora procedemos a configurar los parámetros de búsqueda de hiperparámetros<br><br>
Definimos la **FUNCIÓN DE PÉRDIDA** como MSE o error cuadrático medio, el cual es muy eficiente para los problemas de regresión.<br><br>
También definimos la **METRICA** como MAE o error absoluto medio, media aritmética de las diferencias absolutas entre las predicciones y los valores reales.

(MAPE:  Si se acerca al cero es excelente
MAE: Se tiene que comparar con las unidades de la variable respuesta para saber si es mucho o no. Se necesitan mas datos para interpretarlo. #Si la variable respuesta tiene valores cercanos a cero, el MAPE NO SIRVE. porque no lo puede calcular o lo infla.
)

<hr>

He realizad un cambio de ultima hora, luego de analizar varias funciones de pérdida y varias métricas, dejo el MAE y MAPE

Luego se procede a configurar el tuner con RandomSearch el cual realizará la búsqueda de hiperparámetros. Se define el objetivo de la búsqueda, que es minimizar la métrica especificada (en este caso, minimizar el error absoluto medio)y se establece el número máximo de modelos para probar durante la búsqueda en 20.
"""

#Hiperparámetros de optimización
#Definir función de pérdida y métrica de desempeño

hyperparameters = kt.HyperParameters()
loss = keras.losses.MeanAbsoluteError()
metric_name = "MAPE"
metric = keras.metrics.MeanAbsolutePercentageError(name=metric_name)

tuner = kt.RandomSearch(
    hypermodel=build_credit_model,
    hyperparameters=hyperparameters,
    objective=kt.Objective("val_mean_absolute_percentage_error", direction="min"),
    max_trials=20,
    overwrite=True,
    project_name="credit_card_results",
    max_consecutive_failed_trials=10
)

"""Ahora se pone en acción lo anterior, con 10 epochs. Finalmente se muestra el resumen de los resultados de los modelos evaluados."""

# Realizar la búsqueda de hiperparámetros
tuner.search(X_credit_train, y_credit_train, epochs=10, validation_data=(X_credit_test, y_credit_test))
tuner.results_summary()

"""Tenemos el valor más bajo de error absoluto medio logrado hasta el momento entre todos los modelos evaluados. En este caso, es 7251586. Tenemos el modelo 3 como el mejor asi:  <br><br>

Best val_mean_absolute_percentage_error So Far: 7251586.0<br>
Total elapsed time: 00h 02m 07s<br>
Results summary<br>
Results in ./credit_card_results<br>
Showing 10 best trials<br>
Objective(name="val_mean_absolute_percentage_error", direction="min")<br>
<br><br>
Trial 03 summary<br>
Hyperparameters:<br>
DOR: 0.30000000000000004<br>
opt: adam<br>
Score: 7251586.0<br>

Obviamente este valor está totalmente desfasado por lo que usaremos el afinador por grilla en donde usaremos el optimizador ADAM, ya que combina las ventajas del algoritmo RMSprop y Momentum, ajusta las tasas de aprendizaje de manera adaptativa para cada parámetro y almacena un promedio movil de los gradientes anteriores.<br>

Huber Loss: La pérdida de Huber combina las propiedades del error cuadrático y el error absoluto. Puede ser menos sensible a valores atípicos que el MSE.
"""

#loss=keras.losses.MeanAbsoluteError()
loss = keras.losses.Huber(delta=1.0)

hp=kt.HyperParameters()

m=keras.metrics.MeanAbsoluteError(name="MAE") #Se le da un nombre para facilitar



def model_tuning(hp):
  dr=hp.Float("DR", min_value=0.05, max_value=0.20, step=0.05)
  opti=hp.Choice("OPTI", ['adam','sgd'])
  fa=hp.Choice("FA", ["tanh", "sigmoid"])

#  ann3 = keras.models.Sequential([
#   keras.layers.Input(shape=(X_credit.shape[1],)),
#    keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),
#    keras.layers.Dropout(dr),
#    keras.layers.Dense(64, activation='relu'),
#    keras.layers.Dense(32, activation="relu"),
#    keras.layers.Dense(1, activation="linear")  # Cambiado a 'linear' para problemas de regresión
#  ])

  ann3 = keras.models.Sequential([
    keras.layers.Input(shape=(X_credit.shape[1],)),
    keras.layers.Dense(128, input_shape=(10,),activation=fa, kernel_regularizer=l2),
    keras.layers.Dropout(dr),
    keras.layers.Dense(64, activation=fa, kernel_regularizer=l2),
    keras.layers.Dropout(rate=dr),
    keras.layers.Dense(32, activation=fa, kernel_regularizer=l2),
    keras.layers.Dropout(rate=dr),
    keras.layers.Dense(1, activation='relu', kernel_regularizer=l2),
])

  if opti=="adam":
    opti2=keras.optimizers.Adam(learning_rate=0.001)
  else:
    opti2=keras.optimizers.SGD(learning_rate=0.001)

  ann3.compile(optimizer=opti2, loss=loss, metrics=[m])

  return ann3

"""Estableceremos dos hiperparámetros adicionales que se utilizarán en la construcción del modelo de red neuronal dentro de la función, se trata de rs y L2 para penalizar los pesos grandes en la función de pérdida. Y también establecemos un regularizador L2 con el valor de rs."""

#dr=0.1 #Porcentaje de neuronas a eliminar, que en este caso no lo usaremos
rs=0.01 #Fuerza de penalización de L2

l2=keras.regularizers.l2(rs) #intanciar l2

"""Vamos a usar la búsqueda aleatoria (random search) donde usamos el model tuning definido anteriormente (Dos espacios atrás) y con los parámetros anteriores de regularización. Nuestro objetivo es minimizar, ya que estamos usando como función de pérdida MSE"""

#### hyper parametros de grilla
search_model=kt.RandomSearch(
    hypermodel=model_tuning, ## nombre de funcion de construccion modelo
    hyperparameters=hp,
    objective=kt.Objective('MAE', direction="min"),
    max_trials=10,
    overwrite=True,
    project_name="res_afin",
    max_consecutive_failed_trials=10

)

search_model.search(X_credit_train, y_credit_train, epochs=10, validation_data=(X_credit_test, y_credit_test))
search_model.results_summary()

"""MAE: 990.86474609375

Best MAE So Far: 960.710205078125
Total elapsed time: 00h 02m 07s
Results summary
Results in ./res_afin
Showing 10 best trials
Objective(name="MAE", direction="min")

Trial 05 summary
Hyperparameters:
DR: 0.2
OPTI: adam
FA: tanh
Score: 960.710205078125<br>
Tenemos aproximadamente un error del 20%
<hr>
El resmen seria el siguiente:
"""

search_model.results_summary()

"""Mejor modelo (Trial 06):<br><br>

Dropout Rate (DR): 0.05<br>
Optimizador (OPTI): SGD<br>
Función de Activación (FA): Tanh<br>
MSE (Error Cuadrático Medio): 3,120,201.0<br>

Las combinaciones de hiperparámetros con un Dropout Rate (DR) más bajo y la función de activación tanh junto con el optimizador SGD parece que tuvieran un mejor rendimiento en términos de MSE.

Ahora guardaremos ese resultado en best_credit_model
"""

# Obtener el mejor modelo
best_credit_model = search_model.get_best_models(num_models=1)[0]

best_credit_hps = search_model.get_best_hyperparameters(1)[0]

"""ahora vamos a compilar el mejor modelo de red neuronal con la configuración óptima de hiperparámetros que se encontró durante la búsqueda"""

# Mostrar detalles del mejor modelo
best_credit_model.build()

"""Y este seria el resumen;"""

best_credit_model.summary()

"""En este caso las capas se apilan secuencialmente, con el dropout fuimos apagando algunas capas para prevenir el sobreajuste.
<br>
 Los mejores hiperparámetros encontrados son:  dropout (DR) de 0.05, utiliza el optimizador Stochastic Gradient Descent (sgd) y la función de activación tanh.
"""

# Evaluar el modelo en el conjunto de prueba
y_credit_pred = best_credit_model.predict(X_credit_test)
y_credit_actual = np.array(y_credit_test)
y_credit_pred = np.array(y_credit_pred)[:, 0]

# Mostrar gráficos de evaluación
import sklearn.metrics as metrics
metrics.PredictionErrorDisplay.from_predictions(y_true=y_credit_actual, y_pred=y_credit_pred, kind="actual_vs_predicted")
metrics.PredictionErrorDisplay.from_predictions(y_true=y_credit_actual, y_pred=y_credit_pred, kind="residual_vs_predicted")

mse = mean_squared_error(y_credit_actual, y_credit_pred)
print(f'Mean Squared Error (MSE): {mse}')

mae = mean_absolute_error(y_credit_actual, y_credit_pred)

msle = mean_squared_log_error(y_credit_actual, y_credit_pred)
print(f'Mean Squared Log Error (MSLE): {msle}')

""" MSE: Hay una cantidad significativa de variabilidad no explicada por el modelo.<br>
 MAE: Las predicciones difieren alrededor de 796.88 unidades de la variable objetivo. Esto podria explicarse con los valores tan diferentes que hay, desde 0 hasta 4.900.000.<br>
 MSLE: Este valor es 7.93. El MSLE mide el logaritmo del error cuadrático medio entre los logaritmos naturales de las predicciones y los valores reales.
"""

residuals = y_credit_actual - y_credit_pred
plt.hist(residuals, bins=30)
plt.xlabel("Residuos")
plt.ylabel("Frecuencia")
plt.title("Distribución de Residuos")
plt.show()

"""Podemos observar que la mayoria se acercan a cero, teniendo algunos datos atipicosque llegan hasta 35.000

Exportamos el modelo ganador:
"""

import joblib
joblib.dump(best_credit_model, '/content/drive/MyDrive/Colab Notebooks/ANALITICA 2/TFINAL/win_model.joblib') #erxportar el modelo ganador

# Cargar nuevos datos y hacer predicciones
credit_card_clients2 = pd.read_csv("https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients2.csv")

"""Voy a quitar la columna cust_id para hacer los calculos y luego la vuelvo a agregar"""

cc2  = credit_card_clients2['CUST_ID']

numericas2 = credit_card_clients2.drop(columns=['CUST_ID'])
numericas_escaladas2 = sc.transform(numericas2)

credit_card_clients2_scaled = pd.DataFrame(data=numericas_escaladas2, columns=numericas2.columns)
credit_card_clients2_scaled['CUST_ID'] = cc2

pred_new = best_credit_model.predict(credit_card_clients2_scaled.drop(columns=['CUST_ID']))

credit_card_clients2_scaled['pred'] = pred_new

datos_ordenados = credit_card_clients2_scaled.sort_values(by='pred')

datos_ordenados

top_10_maximo = sorted_data.tail(10)
top_10_maximo

top_10_minimo = sorted_data.head(10)
top_10_minimo