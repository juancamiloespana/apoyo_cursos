# -*- coding: utf-8 -*-
"""Reducción de dimensionalidad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YAxcz9LW4H5zMYmfDjJMZTnMOPuQ0lHl

- Dayana Sofía del Valle
- Laura Camila Betancourt
- Karen Torres

# Librerías #
"""

pip install kneed

# Importamos librerías clásicas
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Librerías para el escalado de datos
from sklearn.preprocessing import StandardScaler

# Librerías para PCA
from sklearn.decomposition import PCA

# Librerías para clustering
from sklearn.neighbors import NearestNeighbors
from sklearn import cluster
from sklearn import mixture
from kneed import KneeLocator


# Ignorar los warnings para tener un código más limpio
import warnings
warnings.filterwarnings("ignore")

"""# Exploración y tratamiento de nulos #

Leemos los datos directamente de github
"""

# Leemos los datos directamente de github
data = pd.read_csv('https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients.csv', index_col=[0]) # Con index_col[0] hacemos que el identificador sea la primera columna del dataset
data

data.info()

"""Vamos a analizar los valores nulos que tiene el dataset, puesto que el algoritmo PCA es susceptible a éstos."""

data.isnull().sum()

"""Se identificó la presencia de valores nulos en dos variables, MINIMUM_PAYMENTS y CREDIT_LIMIT. Debido a la gran cantidad de nulos en MINIMUM_PAYMENTS se llevó a cabo la imputación, donde los valores nulos fueron reemplazados por la media de los datos existentes en la variable, esto se realizó con el fin de mantener la integridad de los datos y evitar la pérdida de información importante. Por otro lado, como la variable CREDIT_LIMIT tiene solo un valor nulo, eliminamos ese registro ya que es insignificante comparado con el tamaño total de los registros."""

data['MINIMUM_PAYMENTS'] = data['MINIMUM_PAYMENTS'].fillna(data['MINIMUM_PAYMENTS'].mean())

data.dropna(inplace = True)

data.isnull().sum()

"""# Reducción de la dimensionalidad #

1. Realice un análisis de reducción de dimensiones e identifique si sería adecuado para el conjunto de datos, defina cuántas variables latentes recomendaría usar y por qué.

Escalamos los datos
"""

df = data.select_dtypes(include=['int', 'float'])

feat_sc = StandardScaler().fit_transform(df)

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, fmt='.2f')
plt.show()

"""Se identificó una alta correlación entre algunas variables, esto puede indicar la presencia de redundancia en los datos, lo que significa que estas pueden proporcionar información similar o casi idéntica, es por esto que se decide realizar reducción de dimensionalidad

Inicialmente ajustamos un PCA con 17 componentes, lo equivalente a las variables que componen nuestra base de datos, para posteriormente aplicar regla del codo.
"""

pca = PCA(n_components=17)
pca.fit(feat_sc)

# porcentaje de varianza explicada
ve = pca.explained_variance_ratio_
ve

# suma de porcentaje de variacion explicada con las  variables latentes
ve.sum()

"""Aplicamos la regla de codo para observar el número de componentes óptimo"""

sns.lineplot(x=np.arange(1,18), y = np.cumsum(ve), palette='viridis')
plt.xlabel('Número de componentes')
plt.ylabel('Varianza explicada')

"""El número de componentes óptimo es 2 para PCA"""

pca = PCA(n_components=2)
pca.fit(feat_sc)

"""**Vectores propios**

Los vectores propios nos indican el peso que tienen cada una de las variables del dataset original en las variables latentes generadas mediante el algoritmo PCA
"""

pca.components_

"""Convertimos estos vectores en un dataframe donde se muestren las variables para comprender cúales están teniendo mayor peso sobre las variables latentes generadas."""

vp = pd.DataFrame(pca.components_)
vp.columns = [i for i in df.columns]
vp

"""**Variable latente 1**
- Las variables originales `PURCHASES: 0.407891` y `PURCHASES_TRX: 0.398314` son las que mayor peso tienen sobre la variable latente 1.

> Estas variables proporcionan información crucial sobre la cantidad y frecuencia de compras de un cliente, lo que influye significativamente en su perfil y en cómo se distingue de otros clientes. Además, estas variables tienen implicaciones en la capacidad de pago del cliente y en su relación con el crédito, lo que las convierte en indicadores clave para caracterizar y agrupar a los clientes en función de sus patrones de gasto y actividad de compra.


**Variable latente 2**
- Las variables originales `CASH_ADVANCE: 0.439584`, `CASH_ADVANCE_FREQUENCY: 0.432393` y `CASH_ADVANCE_TRX: 0.419319` tienen un mayor peso sobre la variable latente 2;

> Estas variables son especialmente importantes en la segunda variable latente debido a su capacidad para revelar el comportamiento de avance de efectivo del cliente. Estas variables proporcionan información esencial sobre la frecuencia y cantidad de avances de efectivo realizados, lo que puede tener un impacto significativo en la estabilidad financiera del cliente y su relación con el crédito.

**Valores propios**
"""

# valores propios = cantidad de varianza explicada
pca.explained_variance_

# porcentaje de varianza explicada
ve = pca.explained_variance_ratio_
ve

# suma de porcentaje de variacion explicada
ve.sum()

"""Los valores propios reflejan la varianza explicada por las variables latentes seleccionadas a partir del total de los datos. En este caso, la primer variable latente explica aproximadamente el 26% de la variabilidad de los datos, mientras que la segunda variable latente explica alrededor del 20%, en conjunto, estas dos variables latentes solo explican el 46% de la variabilidad total de los datos.

La aplicación de PCA resultó en la conservación de solo el 46% de la variabilidad de los datos al reducirlos a dos componentes latentes a partir de un conjunto inicial de 17 variables, esto muestra una pérdida significativa de información. Es por esto que se debe reconsiderar la estrategia de reducción de dimensionalidad y explorar otras alternativas que permitan capturar mejor la estructura de los datos. Si bien aumentar el número de variables latentes es una opción, vale la pena señalar que cada una de las demás variables latentes representan menos del 9% de la variabilidad de los datos, entonces, agregar más variables latentes no sería muy beneficioso en nuestro caso, por lo tanto, podría ser mejor optar por mantener las dos variables latentes o incluso no realizar la reducción de dimensionalidad, dependiendo de la importancia de la información perdida.

**Observamos las variables latentes**
"""

# variables latentes
l = pca.transform(feat_sc)
l

"""Analicemos graficamente los datos

2. Con base en las dos variables latentes más importantes, realice un gráfico y analice qué tipo de clúster se ajustaría mejor de acuerdo con el gráfico. Diga qué porcentaje de varianza explican esos dos componentes y cuáles son las variables observadas que cada variable latente está representando.
"""

sns.scatterplot(x = l[:,0], y = l[:,1], s = 20)
plt.xlabel('Variable latente 1 (Compras)')
plt.ylabel('Variable latente 2 (Avances en efectivo)')
plt.show()

"""El gráfico proporciona una visión conjunta del comportamiento de compras y avances en efectivo de los clientes del banco. Aquí hay algunas observaciones:

- La mayoría de los clientes muestran un bajo nivel tanto de compras como de
avances en efectivo, lo que indica un agrupamiento o mayor densidad hacia los valores bajos en ambas variables.
- Se identifican algunos clientes como puntos atípicos en el extremo derecho del gráfico, quienes tienen un alto nivel de compras pero bajos avances en efectivo.
- Se destacan varios clientes que realizan numerosos avances en efectivo pero pocas compras con tarjeta de crédito.

Analizamos la distribución de las variables latentes para decidir cuál método de clustering debemos aplicar

Dado que nuestras variables latentes muestran una distribución sin formas definidas, optamos por el algoritmo GMM debido a su capacidad para adaptarse de manera más efectiva a la forma de los datos, a diferencia de otros algoritmos de clustering como KMeans, que impone estructuras predefinidas en los datos, GMM es más flexible y puede ajustarse a formas más complejas y no lineales.
"""

# Variables latentes seleccionadas
l_sel=pd.DataFrame(l)

"""# Aprendizaje no supervisado

3. Para agrupar los clientes en segmentos, utilice la técnica de clustering que definió en el punto 2, utilizando las variables latentes o las variables observadas, de acuerdo con lo que definió en el punto 1, y analice gráficamente los clúster con base en dos 2 variables latentes (incluso si estas no fueron utilizadas para hacer el clúster).

## GMM
"""

from sklearn.model_selection import GridSearchCV
from sklearn.mixture import GaussianMixture

# modelo
gmm = GaussianMixture(n_components=5, covariance_type='full', n_init=5)
# ajustar
gmm.fit(l_sel)
gmm.aic(l_sel) ## indicador para comparar con otro modelo. Por si solo no se interpreta
gmm.bic(l_sel) ## indicador para comparar con otro modelo. Por si solo no se interpreta
gmm.score(l_sel) ## indicador de verosimilitud
gmm.predict_proba(l_sel)[0] # Fila cero, probabilidad de pertenecia a cada uno de los 5 cluster

# grilla de hiperparametros
paramg = {
    'n_components':[2,3,4,5,6,7,8],
    'covariance_type':['full', 'tied', 'spherical', 'diag']
}

# Funcion para optimizar hiperparametros
gs = GridSearchCV(gmm, param_grid=paramg)
# ajustar modelo
gs.fit(l_sel)

# Resultados
df_resultados = pd.DataFrame(gs.cv_results_)
df_resultados[['params', 'mean_test_score']].sort_values('mean_test_score', ascending = False)

# Mejor resultado
gmm_gan=gs.best_estimator_
# guardar los cluster en una variable
cluster_gmm = gmm_gan.fit_predict(l_sel)
# graficar hue=categorias o clusteres
sns.scatterplot(x=l_sel.iloc[:,0], y=l_sel.iloc[:,1], hue=cluster_gmm, palette='husl')

"""# Analisis de los clusters"""

# crear una copia de la base original
df2 = df.copy()
# agregar la variable con los clusteres
df2['cluster'] = cluster_gmm
df2

# Calcular estadísticas descriptivas para cada cluster
cluster_stats = df2.groupby('cluster').describe()
# Filtrar estadísticas: media (mean), mínimo (min) y máximo (max)
cluster_stats_filtered = cluster_stats.loc[:, (slice(None), ['mean', 'std', 'min', 'max'])]
cluster_stats_filtered

# Filtrar estadísticas para la media
mean_stats = cluster_stats_filtered.xs('mean', axis=1, level=1)

# Crear subgráficos para ambas visualizaciones
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Gráfico de barras para la media de compras por cluster
sns.barplot(data=mean_stats.reset_index(), x='cluster', y='PURCHASES', palette='husl', ax=axes[0])
axes[0].set_xlabel('Cluster')
axes[0].set_ylabel('Media')
axes[0].set_title('Media de compras por cluster')

# Gráfico de barras para la media de avances por cluster
sns.barplot(data=mean_stats.reset_index(), x='cluster', y='CASH_ADVANCE', palette='husl', ax=axes[1])
axes[1].set_xlabel('Cluster')
axes[1].set_ylabel('Media')
axes[1].set_title('Media de avances por cluster')

# Mostrar los subgráficos
plt.tight_layout()
plt.show()

"""Se analizan las medias de las variables con mayor peso en las variables latentes una esta relacionada con la cantidad de compras y la otra con la cantidad de avances realizados:

Cluster 0: En este segmento las compras son casi nulas en a los avances con la tarjeta de credito, los avances son casi nulos en este cluster.

Cluster 1: Tanto compras es casi nulos en comparación con los avances en este cluster, por lo cual el segmento alberga a clientes que prefieren hacer avances pero de poco dinero.

Cluster 2: Las personas segmentadas en el cluster 2 realizan mayor uso de su tarjeta de credito para compras que para avances

Cluster 3: En este segmento de clientes el uso de las tarjetas para compras es menor a 2000, pero para avances el promedio es superior a 5000.

Cluster 4: En este cluster se albergan las personas que hacen una gran cantidad tanto de compras como de avances, lo que demuestra que tienen un gran uso de sus tarjetas de credito.

Cluster 5: En este grupo de personas se evidencia un uso muy bajo de las tarjetas para ambas actividades.

Cluster 6: En este cluster se realizan pocas compras, sin embargo su media para el caso de los avances es buena.

Cluster 7: Muestra que se realizaron más compras que avances, aún así las medias de ambas son buenas.

4. Con base en los clústeres identificados en el punto anterior, haga una descripción de las características de cada clúster y defina estrategias que le propondría la empresa para cada clúster (de acuerdo con las características identificadas).

A partir de la segmentación realizada proponemos las siguientes estrategias para los de usuarios de tarjetas de crédito:

**Cluster 0:** Mínima actividad en compras y avances.
- Estrategia: Incentivar el uso de la tarjeta de crédito con ofertas especiales, descuentos u programas de recompensas por compras realizadas.

**Cluster 1:** Avances en efectivo en cantidades pequeñas.
- Estrategia: Educar a los clientes sobre las ventajas de utilizar la tarjeta para compras en lugar de avances en efectivo, destacando beneficios como acumulación de puntos o reembolsos.

**Cluster 2:** Uso principal para compras, poco en avances.
- Estrategia: Mantener programas de fidelización que recompensen las compras y promover ofertas especiales en categorías específicas de productos.

**Cluster 3:** Bajo uso en compras, pero significativo en avances.
- Estrategia: Identificar las razones detrás de la baja actividad en compras y ofrecer incentivos para fomentar un mayor uso de la tarjeta en esa área, al tiempo que se controla el uso de avances en efectivo.

**Cluster 4:** Alto uso tanto en compras como en avances.
- Estrategia: Brindar programas de gestión de deudas y educación financiera para ayudar a los clientes a administrar su uso de la tarjeta y evitar el endeudamiento excesivo.

**Cluster 5:** Actividad muy baja en ambos.
- Estrategia: Implementar campañas de marketing dirigidas para reactivar la participación de estos clientes, ofreciendo incentivos especiales o promociones personalizadas.

**Cluster 6:** Pocas compras, pero buena media en avances.
- Estrategia: Analizar las razones detrás de la baja actividad en compras y ofrecer incentivos específicos para aumentarlas, al mismo tiempo que se controla el uso de avances en efectivo.

**Cluster 7:** Mayor actividad en compras que en avances, con buenas medias en ambas.
- Estrategia: Mantener la satisfacción de estos clientes ofreciendo recompensas adicionales por compras continuas y promoviendo ofertas especiales que aprovechen su comportamiento de gasto.
"""