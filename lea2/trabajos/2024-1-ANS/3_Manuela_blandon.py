# -*- coding: utf-8 -*-
"""Trabajo aprendizaje no supervisado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MAbBjG-kOH3vRFW2SVH796Ba7uNgm-aU

#**Estudiantes:** Manuela Blandón y Karen Marcela Pérez

Diccionario de datos

CUST_ID: Identificación del titular de la tarjeta de crédito

BALANCE: Saldo disponible para compras

BALANCE_FREQUENCY: Frecuencia de actualización del saldo (donde 1 = actualización frecuente y 0 = no se actualiza con frecuencia).

PURCHASES: Importe de las compras realizadas por el cliente

ONEOFF_PURCHASES: Importe máximo de la compra realizada a una couta

INSTALLMENTS_PURCHASES: Importe de las compras realizadas a plazos

CASH_ADVANCE: Avances de Efectivo realizados por el usuario

PURCHASES_FREQUENCY: Frecuencia con la que se realizan las compras (1 = compras frecuentes, 0 = compras poco frecuentes)

ONEOFF_PURCHASES_FREQUENCY: frecuencia con la que se realizan las compras a una couta (1 = compras frecuentes, 0 = compras poco frecuentes)

PURCHASES_INSTALLMENTS_FREQUENCY: Frecuencia con la que se realizan compras a plazos (1 = se realizan con frecuencia, 0 = no se realizan con frecuencia)

CASH_ADVANCE_FREQUENCY: Frecuencia con la que se realizan avances de efectivo

CASH_ADVANCE_TRX: Número de transacciones realizadas por "avances de efectivo"

PURCHASES_TRX: Número de transacciones de compra realizadas

CREDIT_LIMIT: Límite de la tarjeta de crédito del usuario

PAYMENTS: Importe de los pagos realizados por el usuario

MINIMUM_PAYMENTS: Importe mínimo de los pagos realizados por el usuario

PRC_FULL_PAYMENT: Porcentaje del pago total abonado por el usuario

TENURE: Tenencia del servicio de tarjeta de crédito para el usuario

# Librerías
"""

import seaborn as sns ### para los datos y para gráficar
import matplotlib.pyplot as plt
from sklearn import cluster ### modelos de clúster
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
!pip install kneed
from sklearn.metrics import silhouette_score ## indicador para consistencia de clúster
from kneed import KneeLocator ### para detectar analíticamente el cambio en la pendiente

"""# **Base de Datos**"""

# Cargar el archivo CSV en un DataFrame, sin especificar una columna como índice
df = pd.read_csv('https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients.csv', index_col=0)
df

df.info() #verificación de nulos, no hay nulos

"""#Análisis exploratorio"""

df.shape

valores_nulos_por_columna = df.isnull().sum()

# Comprueba si hay valores nulos en alguna parte del DataFrame
hay_nulos = df.isnull().values.any()

# Imprime la cantidad de valores nulos por columna y si hay valores nulos en el DataFrame
print("Cantidad de valores nulos por columna:")
print(valores_nulos_por_columna)
print("\n¿Hay valores nulos en el DataFrame?", hay_nulos)

"""Hay 314 datos faltantes en total, como la base de datos es de 8950 datos, se procede a eliminar estos"""

df.columns

import plotly.graph_objects as go
from plotly.subplots import make_subplots

colors = ['#FF7F50', '#87CEEB', '#32CD32', '#800080', '#FFA500', '#FFC0CB', '#A52A2A', '#F08080', '#5F9EA0']
fig = make_subplots(rows=len(df.columns), cols=2)
columns = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES',
       'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY']

# Configurar el diseño de la figura
fig.update_layout(
    autosize=False,
    width=800,  # Ancho total de la figura
    height=600 * len(columns),
    title_text="Histogramas y Boxplots de Variables Numéricas"
)

# Iterar sobre cada columna de x_numericas
for i, column in enumerate(columns):
    # Añadir histograma
    fig.add_trace(
        go.Histogram(x=df[column], name=f"Histograma de {column}", marker_color=colors[i]),
        row=i+1, col=1  # Añadir el histograma en la fila i+1, columna 1
    )
    # Añadir boxplot
    fig.add_trace(
        go.Box(y=df[column], name=f"Boxplot de {column}", marker_color=colors[i]),
        row=i+1, col=2
    )

    fig.update_xaxes(title_text=column, row=i+1, col=1)
    fig.update_yaxes(title_text=column, row=i+1, col=1)

# Mostrar la figura
fig.show()

colors = ['#FF7F50', '#87CEEB', '#32CD32', '#800080', '#FFA500', '#FFC0CB', '#A52A2A', '#F08080', '#5F9EA0']
fig = make_subplots(rows=len(df.columns), cols=2)
columns = ['PURCHASES_INSTALLMENTS_FREQUENCY',
       'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',
       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT',
       'TENURE']

# Configurar el diseño de la figura
fig.update_layout(
    autosize=False,
    width=800,  # Ancho total de la figura
    height=600 * len(columns),
    title_text="Histogramas y Boxplots de Variables Numéricas"
)

# Iterar sobre cada columna de x_numericas
for i, column in enumerate(columns):
    # Añadir histograma
    fig.add_trace(
        go.Histogram(x=df[column], name=f"Histograma de {column}", marker_color=colors[i]),
        row=i+1, col=1  # Añadir el histograma en la fila i+1, columna 1
    )
    # Añadir boxplot
    fig.add_trace(
        go.Box(y=df[column], name=f"Boxplot de {column}", marker_color=colors[i]),
        row=i+1, col=2
    )

    fig.update_xaxes(title_text=column, row=i+1, col=1)
    fig.update_yaxes(title_text=column, row=i+1, col=1)

# Mostrar la figura
fig.show()

"""En general se observan la mayoría de las variables con datos atípicos. Sin embargo no se procede a imputar o eliminarlos debido a que puede deberse al contexto (tarjetas de créditos), y por lo tanto pueden haber altas variaciones entre los datos que depende de aspectos como el estrato, salario, calidad de vida del cliente, entre otros. Además, por temas de desconocimiento de la base de datos y el tiempo es límitado para analizar los atípicos, se procede a usar la base de datos sin tratarlos.

#Eliminación de nulos

Se eliminan los datos faltantes
"""

df = df.dropna()

df.shape

"""# **1.Reducción de dimensiones: Análisis de Componentes Principales**

## Librerías RD
"""

from sklearn.decomposition import PCA
!pip install factor-analyzer
from factor_analyzer import FactorAnalyzer

"""## Estandarización de los datos"""

x_numericas = df.copy()

sc= StandardScaler().fit(x_numericas)
feat_sc=sc.transform(x_numericas) #escalado de esas  variables

"""## PCA"""

pca=PCA(n_components=17) ## se puede dar valor de componentes o varianza explicada
pca.fit(feat_sc)

"""Variables latentes y lambdas"""

# Nombres de las variables originales
nombres_variables = []
for i in x_numericas.columns:
    nombres_variables.append(i)

print(nombres_variables)

# Crear un DataFrame con los componentes y los nombres de las variables
df_componentes = pd.DataFrame(data=pca.components_, columns=nombres_variables)

# Añadir una columna para el número de componente
df_componentes.insert(0, 'Componente', range(1, len(pca.components_)+1))

# Imprimir el DataFrame
round(df_componentes.head(),4)

pca.explained_variance_ ## valores propios alpha, cuánta varianza es explicada

ve=pca.explained_variance_ratio_ ### procentaje de variable explicada por cada componente
ve #en menos variables se tiene la mayor parte de la variable explicada, por lo tanto se podría hacer reducción de dimensiones

df_var_explicada = pd.DataFrame({
    'Número de Componente': range(1, len(ve) + 1),
    'Proporción de VE': ve
})
round(df_var_explicada,4)

plt.figure(figsize=(6, 4))
plt.bar(range(1, len(ve) + 1), ve, color='skyblue')
plt.xlabel('Componente Principal')
plt.ylabel('Proporción de Varianza Explicada')
plt.title('Proporción de Varianza Explicada por Componente Principal')
plt.xticks(range(1, len(ve) + 1))
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

l = pca.transform(feat_sc) ## variables latentes
l[0] ## variables latentes para primera fila
pd.DataFrame(l)

#### analizar número de componentes con regla del codo
sns.lineplot(x=np.arange(1,18), y=np.cumsum(ve), palette="viridis")

"""Como no se alcanza a apreciar en la gráfica el número de componentes principales, se procede a hallar el valor analíticamente"""

kl=KneeLocator(x=np.arange(1,18), y=np.cumsum(ve),curve="concave", direction="increasing")
kl.elbow

sum = 0
for i in range(0,7):
    sum = sum + ve[i]

sum*100

"""## Análisis  punto 1

Se comprueba con la prueba analítica del codo que las 7 primeras variables latentes explican más de 80% de la varianza de los datos, por lo tanto se justifica la realización de reducción de dimensiones, al tomar estas variables.

# Punto 2: Elección del método de clusterización
"""

#Se seleccionan las dos variables latentes más importantes.
l_sel=l[:,0:2]
sns.scatterplot(x=l_sel[:,0], y=l_sel[:,1])

plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('Scatterplot de Variables Latentes')
plt.show()

sum = 0
for i in range(0,2):
    sum = sum + ve[i]

sum*100

"""Según el gráfico utilizando las dos primeras variables latentes se usará un GMM este puede ser más útil teniendo en cuenta la forma en que se dispersa los datos o se distribuyen. Es decir puede amoldar o ser más flexible a la forma que tiene la distribución.
La varianza explicado de los dos componentes principales es de casi el 47%.

En este caso, cada valor representaría los lambdas del peso que tienen las variables originales sobre las originales latentes.

Componente 1:
* PURCHASES 0.41
* ONEOFF_PURCHASES 0.31
* INSTALLMENTS_PURCHASES 0.35
* PURCHASES_FREQUENCY 0.34
* ONEOFF_PURCHASES_FREQUENCY 0.3
* PURCHASES_INSTALLMENTS_FREQUENCY 0.3
* PURCHASES_TRX 0.40
* CREDIT_LIMIT 0.21
* PAYMENTS 0.26

Componente 2:
* BALANCE: 0.41
*   CASH_ADVANCE 0.44
* CASH_ADVANCE_FREQUENCY 0.43
* CASH_ADVANCE_TRX 0.42
* CREDIT_LIMIT 0.25
* PAYMENTS 0.27
* PRC_FULL_PAYMENT 0.2

# Punto 3: GMM
"""

from sklearn import mixture
from sklearn.model_selection import GridSearchCV

sns.scatterplot(x=l_sel[:,0], y=l_sel[:,1])
gmm=mixture.GaussianMixture(n_components=3, covariance_type='full', n_init=5)

gmm.fit(l_sel)
gmm.score(l_sel) # score por defecto es logaritmo de la función de verosimilitud es adiminesional y no se interpreta por sí solo
gmm.predict_proba(l_sel) # probabilidad de pertecencer a cada cluster
gmm.predict(l_sel) # para conocer los label del cluster.
gmm.bic(l_sel)

#Grilla de hiperparámetros
param_grid = {
    'n_components': [1,2,3],
    'covariance_type': ['full', 'diag', 'tied', 'spherical'],
    'n_init': [5]
}

gs = GridSearchCV(gmm, param_grid=param_grid)
gs.fit(l_sel)

gs.best_params_

gmm_win = gs.best_estimator_
gmm_win

cluster_label_gmm = gmm_win.predict(l_sel)
gmm_win.predict_proba(l_sel)

sns.scatterplot(x=l_sel[:, 0], y=l_sel[:, 1], hue=cluster_label_gmm, palette='viridis') #También se optó por n_components o clusters de 3
plt.title("clusters de acuerdo a gmm para componentes principales")
plt.show()

"""##Análisis gráfico de los cluster con variables latentes
Se clasifican los datos en 3 segmentaciones, el cluster 0 y 1 son de un tamaño similar, y el cluster 2 es el de mayor tamaño y quien incluye los datos atípicos o los que están más alejados de la mayoría de los datos.
Además, al ser componentes principales la interpretación gráfica puede ser un poco abstracta o compleja debido al proceso que hay detrás para poder llegar a la segmentación (combinación lineal). Por ende, se procederá a realizar un análisis con las dos variables originales que mayor peso tienen en cada uno de los componentes principales seleccionados para conocer a fondo las características de los clusters.

# Punto 4: Análisis de los clusters con las variables originales
"""

df['cluster'] = cluster_label_gmm
df.head()

d = df.groupby('cluster').describe()
estadisticas = d.loc[:, (slice(None), ['mean', 'std', 'min', 'max'])]
estadisticas

"""Se escogen las variables PURCHASES y CASH_ADVANCE, ya que son las que tienen mayor peso en las 2 primeras componentes principales."""

# Filtrar estadísticas para la media

mean_stats = estadisticas.xs('mean', axis=1, level=1)

# Crear subgráficos para ambas visualizaciones
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Gráfico de barras para la media de compras por cluster
sns.barplot(data=mean_stats.reset_index(), x='cluster', y='PURCHASES', palette='viridis', ax=axes[0])
axes[0].set_xlabel('Cluster')
axes[0].set_ylabel('Media')
axes[0].set_title('Media de compras por cluster')

# Gráfico de barras para la media de avances por cluster
sns.barplot(data=mean_stats.reset_index(), x='cluster', y='CASH_ADVANCE', palette='viridis', ax=axes[1])
axes[1].set_xlabel('Cluster')
axes[1].set_ylabel('Media')
axes[1].set_title('Media de avance de dinero por cluster')

# Mostrar los subgráficos
plt.tight_layout()
plt.show()

features = ['PURCHASES', 'CASH_ADVANCE']

plt.figure(figsize=(10, 5))
for i, feature in enumerate(features):
    plt.subplot(1, 2, i+1)
    sns.boxplot(x='cluster', y=feature, data=df, palette='viridis')
    plt.title(f'{feature} por Cluster')

plt.tight_layout()
plt.show()

df['cluster'].value_counts()

"""Características de cada cluster:

**Cluster 0**
- Están los clientes que tienen una media de  compra de aproximadamente 1000 dólares y una media de menos de 500 dólares de avance de efectivo. Por lo tanto, podemos afirmar que en este clúster están las personas que realizan los menores avances pero con compras significativas.

**Cluster 1**
-  En este cluster las compras en promedio son de mucho menos de 500 dólares, pero los avances de efectivo son sustanciales, siendo casi de 1500 dólares. Esto tiene sentido porque las personas al hacer retiros de efectivo tienenden a disminuir su monto de la tarjeta de crédito, y por ende realizar una proporción de compras menores.

**Cluster 2**
- En este cluster se encuentran los clientes que utilizan la tarjeta para compras y para realizar avances con un promedio de 3000 dólares. La característica de estas perswonas puede deberse a que necesitan flujo de efectivo y a su vez realizan los pago o compras con tarjetas de crédito.

Estrategias:

1. Estrategías ligadas a las características de las personas del cluster 0: Ofrecer incentivos o informar a los clientes sobre las ventajas que traen los avances en efectivo vs compras regulares. Esto para aumentar la proporción de uso de la tarjeta para avances.

2.  cluster 1: Generar programas para que las personas utilicen más las compras con las tarjetas de crédito, además de realizar avances.

3. Cluster 2: Para esos clientes estrellas que utilizan la tarjeta de crédito para compras y avances en efectivo generar un programa de fidelización por medio de premios o bonos para seguir incentivando su uso.

Recomendaciones para futuros proyectos:
Dado que no se realizó un análisis exploratorio detallado, y no se realizó ningún tipo de tratamiento para los datos atípicos, la solución o las propuestas dadas pueden estar un poco sesgadas, ya que el algoritmo de PCA utilizado en la reducción de dimensiones es sensible a estos datos.
"""