# -*- coding: utf-8 -*-
"""Redes Neuronales - Parte 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vAx7JeO0KmJ5R8pGBO3wlqlk45NLSog7

```
Laura Betancourt
Karen Torres
Sofia del Valle
```

**Segunda parte**
"""

##basicas
import pandas as pd
import numpy as np


####### redes neuronales

import tensorflow as tf
from tensorflow import keras
from sklearn import metrics

##########
import joblib
import scipy.stats as stats

from google.colab import drive
drive.mount('/content/drive')

"""Importar objetos"""

sc2 = joblib.load('/content/drive/MyDrive/Colab Notebooks/sc.joblib')

modelo_win= joblib.load('/content/drive/MyDrive/Colab Notebooks/win_model.joblib')
modelo_win.summary()

x_tr = joblib.load('/content/drive/MyDrive/Colab Notebooks/x_tr.joblib')
x_te = joblib.load('/content/drive/MyDrive/Colab Notebooks/x_te.joblib')
y_tr = joblib.load('/content/drive/MyDrive/Colab Notebooks/y_tr.joblib')
y_te = joblib.load('/content/drive/MyDrive/Colab Notebooks/y_te.joblib')

"""## Punto 1

**Análisis del modelo**
"""

pred_te = modelo_win.predict(x_te)
pred_te.shape

y_act=np.array(y_te)[:,0]
y_act.shape

y_pred=np.array(pred_te)[:,0]
y_pred.shape

import sklearn.metrics as metrics

metrics.PredictionErrorDisplay.from_predictions(y_true=y_act,y_pred=y_pred, kind="actual_vs_predicted")
metrics.PredictionErrorDisplay.from_predictions(y_true=y_act,y_pred=y_pred, kind="residual_vs_predicted")

"""En el gráfico de predicciones vs valores reales, se observa que hay una cantidad significativa de puntos cercanos a la línea diagonal, esto puede significar que las predicciones del modelo son bastante precisas, sin embargo, hay una dispersión evidente en los valores más altos, lo que nos puede desir también que el modelo no está muy bien ajustado, o sea, que puede estar subajustado o sobreajustado.


En el gráfico de los residuos, se observa que la dispersión va aumentando a medida que aumentan los valores predichos, entonces, la variabilidad de los errores no es constante, así mismo, algunos residuos son bastante grandes, más que todo, en valores más altos, es por esto que se puede decir que el modelo tiene dificultades para predecir de forma correcta.

Conclusiones:



*   En general, el modelo tiene un buen rendimiento, ya que muchas predicciones están cerca de los valores reales.

*   Hay mayor probabilidad de que las predicciones de valores altos no sean tan precisas, debido a que los errores son mayores para predicciones de valores altos.
*   Se puede considerar hacer el modelo más complejo para que tenga más en cuenta la variabilidad de los datos.

Carga de datos 2
"""

url2 = 'https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients2.csv'
df2 = pd.read_csv(url2, index_col=[0])
df2

display(df2.isnull().sum())
df2.dropna(inplace = True)

df2.isnull().sum()

x_sc2=sc2.transform(df2)

"""## **Punto 2**

**Predicción de las compras en el periodo Jul-Dic 2023**
"""

#Predicción del numero de compras de cada cliente

pred_new=modelo_win.predict(x_sc2)
data = pd.DataFrame(pred_new)
data.columns = ['predicciones']
data

data.describe()

"""El valor promedio de las predicciones es aproximadamente 812 compras, sugiriendo que, en promedio, cada cliente realizará alrededor de 812 compras. Sin embargo, la desviación estándar es alta (aproximadamente 1074), indicando una gran variabilidad en las predicciones del modelo. Por su parte, el 25% de las predicciones están por debajo de aproximadamente 24.47 compras, mostrando que un cuarto de los clientes están predichos a realizar menos de 24 compras. Por otro lado, el 75% de las predicciones están por debajo de aproximadamente 1137.46 compras, indicando que solo el 25% de los clientes están predichos a realizar más de 1137 compras."""

import matplotlib.pyplot as plt

plt.hist(data['predicciones'], bins=50)
plt.xlabel('Número de Compras Predichas')
plt.ylabel('Frecuencia')
plt.title('Distribución de las Predicciones de Compras')
plt.show()

"""En cuanto a la distribución de los datos se puede observar un sesgo a la derecha. Esto quiere decir que la mayoría de las predicciones se encuentran en los rangos más bajos, con una gran cantidad de valores cercanos a 0. Por otro lado, la frecuencia disminuye rápidamente a medida que el número de compras predichas aumenta. Esto sugiere que el modelo predice que la mayoría de los clientes realizarán un número bajo de compras y solo unos pocos realizarán un número alto de compras.

**Top 10 de clientes con mayor número de compras**
"""

sorted_pred_nuev = sorted(pred_new, reverse=True)

# Convertir la lista ordenada en un DataFrame
df = pd.DataFrame({'predicciones': sorted_pred_nuev})

# Imprimir el DataFrame resultante
top= df.head(10)
top

top = pd.DataFrame(top)

# Convertir 'predicciones' a valores escalares si es necesario
top['predicciones'] = top['predicciones'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)

mean_top = np.mean(top['predicciones'])
desv_std = np.std(top['predicciones'], ddof=1)  # ddof=1 es la desviación estándar muestral

z = 0.80
alfa = 1 - z
z_critico = stats.norm.ppf(1 - alfa/2)

# Calcular el error estándar
n = len(top)
error_estandar = desv_std / np.sqrt(n)

# Calcular el intervalo de confianza
margen_error = z_critico * error_estandar
intervalo_confianza = (mean_top - margen_error, mean_top + margen_error)

print(f"Media de la muestra: {mean_top}")
print(f"Desviación estándar de la muestra: {desv_std}")
print(f"Valor crítico (z): {z_critico}")
print(f"Error estándar: {error_estandar}")
print(f"Margen de error: {margen_error}")
print(f"Intervalo de confianza del 80%: {intervalo_confianza}")

# Utilizar .loc para evitar SettingWithCopyWarning
top.loc[:, 'Cota inferior'] = top['predicciones'] - margen_error
top.loc[:, 'Cota superior'] = top['predicciones'] + margen_error

top

"""Los 10 clientes que se espera que hagan más compras en el periodo Jul-Dic 2023 son los que se muestran en el dataframe anterior, con el cliente 5086 a la cabeza con 7516.98 compras predichas, seguido por los clientes 1166 y 1509 con más de 6200 compras cada uno. Para calcular el intervalo de confianza del 80% de estas predicciones, se estimó un margen de error de aproximadamente 14.8. Este margen de error es bastante estrecho, lo que significa que nuestras predicciones son bastante precisas y la variabilidad alrededor de las estimaciones es pequeña. Esto nos da una alta confianza en las predicciones.

## **Punto 3**

**Estrategias**

Estrategias:
* Identificar a los clientes predichos a realizar un alto número de compras y clasificarlos como clientes de alto valor, para así crear programas de lealtad específicamente diseñados para estos clientes, ofreciéndoles beneficios exclusivos como puntos de recompensa adicionales, descuentos personalizados etc.
* Identificar los productos o categorías preferidas por los clientes y ofrecer promociones y descuentos en estos productos para incentivar compras adicionales.
* Ofrecer incentivos específicos para reactivar a los clientes con pocas o cero compras, como un alto porcentaje de cashback en las primeras compras después de la reactivación o descuentos exclusivos en sus categorías de interés.
* Crear promociones por tiempo limitado para motivar a los clientes a usar la tarjeta, como ofertas de "compra y recibe" o descuentos adicionales en productos populares.
"""