# -*- coding: utf-8 -*-
"""Trabajo DeepLearning Parte 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17sZpE1_wc-FiDu-qONnJnlmV6RAwCzzq

#Nombres:
Manuela Blandón

Karen Pérez

#Diccionario de datos

CUST_ID: Identificación del titular de la tarjeta de crédito

BALANCE: Saldo disponible para compras

BALANCE_FREQUENCY: Frecuencia de actualización del saldo (donde 1 = actualización frecuente y 0 = no se actualiza con frecuencia).

PURCHASES: Importe de las compras realizadas por el cliente

ONEOFF_PURCHASES: Importe máximo de la compra realizada a una couta

INSTALLMENTS_PURCHASES: Importe de las compras realizadas a plazos

CASH_ADVANCE: Avances de Efectivo realizados por el usuario

PURCHASES_FREQUENCY: Frecuencia con la que se realizan las compras (1 = compras frecuentes, 0 = compras poco frecuentes)

ONEOFF_PURCHASES_FREQUENCY: frecuencia con la que se realizan las compras a una couta (1 = compras frecuentes, 0 = compras poco frecuentes)

PURCHASES_INSTALLMENTS_FREQUENCY: Frecuencia con la que se realizan compras a plazos (1 = se realizan con frecuencia, 0 = no se realizan con frecuencia)

CASH_ADVANCE_FREQUENCY: Frecuencia con la que se realizan avances de efectivo

CASH_ADVANCE_TRX: Número de transacciones realizadas por "avances de efectivo"

PURCHASES_TRX: Número de transacciones de compra realizadas

CREDIT_LIMIT: Límite de la tarjeta de crédito del usuario

PAYMENTS: Importe de los pagos realizados por el usuario

MINIMUM_PAYMENTS: Importe mínimo de los pagos realizados por el usuario

PRC_FULL_PAYMENT: Porcentaje del pago total abonado por el usuario

TENURE: Tenencia del servicio de tarjeta de crédito para el usuario

#Librerias
"""

###basicas
import pandas as pd
import numpy as np

##### datos y modelos sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

####### redes neuronales

import tensorflow as tf
from tensorflow import keras

####instalar paquete
!pip install keras-tuner

import keras_tuner as kt ### paquete para afinamiento
import seaborn as sns

import joblib

"""#Carga de datos"""

url="https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients.csv"
credit_card=pd.read_csv(url)

url="https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients2.csv"
credit_card2=pd.read_csv(url)

print(credit_card.shape)
print(credit_card2.shape)

print(credit_card.info())
print(credit_card2.info())

"""Para entrenar un modelo que predice las compras de los clientes se utilizará el dataframe credit_card_clients, ya que el otro no contiene la variable Purchase, y esta sería nuestra variable dependiente.

#Verificación de nulos
"""

credit_card.info()

credit_card.isnull().sum()

"""Hay 313 y 1 datos nulos en las variables MINIMUM_PAYMENTS y CREDIT_LIMIT respectivamente que se eliminarán debio al tiempo y que son una proporción muy baja de los datos. Además, se eliminará CUST_ID ya que no aporta información importaante al modelo

"""

credit_card = credit_card.dropna()
credit_card.isnull().sum()
credit_card = credit_card.drop('CUST_ID', axis=1)

credit_card.isnull().sum()

"""#Punto 1

##Separar variables
"""

y = credit_card['PURCHASES']
X = credit_card.drop('PURCHASES',axis=1)
X

"""##Escalar datos"""

sc=StandardScaler().fit(X) ## calcular la media y desviacion para hacer el escalado
X_sc=sc.transform(X)  ## escalado con base en variales escladas
X_sc

"""##Train y Test"""

X_tr, X_te, y_tr, y_te= train_test_split(X_sc, y, test_size=0.2)
X_tr.shape

"""## Arquitectura 1

La siguiente arquitectura se define con 3 capas neuronales densas, de 2 ocultas y una capa de salida. Se utiliza la función de activación relu porque es la función que más se podría adaptar al comportamiento de los datos. Se va a ensayar por cada capa con un número bajo de neuronas. La última capa se deja 1 neurona porque se trata de un problema de regresión
"""

ann2=keras.models.Sequential([
    keras.layers.InputLayer(input_shape=X_tr.shape[1:]),
    keras.layers.Dense(units=16, activation='relu'),
    keras.layers.Dense(units=8, activation='relu'),
    keras.layers.Dense(units=1, activation='relu')
])
print(ann2.count_params())

"""Se utilizará la metrica RMSE por su fácil interpretación. Además se propone que el valor del RMSE no sea mayor al 40% de la media de la variable de respuesta.

El RMSE es solo la raíz cuadrada de MSE. La raíz cuadrada se
introduce para hacer que la escala de los errores sea igual
a la escala de los objetivos. Este es análogo a la desviación
estándar, mientras el MSE a la varianza

Ventajas:
*   RMSE es fácil de entender.
*   RMSE no penaliza los errores tanto como lo
hace MSE debido a la raíz cuadrada.



"""

l = keras.losses.MeanSquaredError()  # Función de pérdida para regresión
m = keras.metrics.RootMeanSquaredError(name ='RMSE')  # Métrica para regresión

ann2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=l, metrics=m)
##### configurar el fit ###
ann2.fit(X_tr, y_tr, epochs=10, validation_data=(X_te, y_te))

media = y.mean()
media

print(f'{round(( 781.4403/media)*100,2)}%')
print(f'{round(( 693.1174/media)*100,2)}%')

"""##  Nueva Arquitectura

Se decidió por complejizar el modelo debido a que se presenta underfitting ya que el error promedio de lo que se predice de los datos de train y del test son del 76.21% y 67.59% respectivamente. Para ello se implementará un aumento en las capas, además de la cantidad de neuronas, el número de epoch a 25 y a una tasa de aprendizaje de 0.001
"""

ann2=keras.models.Sequential([
    keras.layers.InputLayer(input_shape=X_tr.shape[1:]),
    keras.layers.Dense(units=512, activation='relu'),
    keras.layers.Dense(units=256, activation='relu'),
    keras.layers.Dense(units=128, activation='relu'),
    keras.layers.Dense(units=64, activation='relu'),
    keras.layers.Dense(units=1, activation='relu')
])
print(ann2.count_params())

l = keras.losses.MeanSquaredError()  # Función de pérdida para regresión
m = keras.metrics.RootMeanSquaredError(name ='RMSE')  # Métrica para regresión

ann2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=l, metrics=m)
##### configurar el fit ###
ann2.fit(X_tr, y_tr, epochs=15, validation_data=(X_te, y_te))

print(f'{round(( 568.7534/media)*100,2)}%')
print(f'{round(( 510.8511/media)*100,2)}%')

"""Como la meta propuesta fue del 40%, y los valores que se obtuvieron están cercanos a este valor, se decide dejar este algoritmo para entrenar el modelo por temas de tiempo.

## Hiperparámetros
"""

hp=kt.HyperParameters()
hp

"""Afinamiento de hiperparámetro con el último algoritmo utilizado.

A continuación se presenta la función model_tuning(hp), la cual recibe como argumento los hiperparámetros del keras tuner, estos se describen a continuación:

*   DOR:Dropout_rate: porcentaje de las neuornas a eliminar (entre 0 y 1).
*   SR: Regularization_streght: Peso de la penalización que se le aplica a la función de pérdida con base en los parámetros.
*   FA_CO: función de activación
*   opt: Utiliza algoritmos para optimizar las redes neuronales, los algoritmos utilizados son: Adam y SGD.

Luego, sea ajuta la regularización kera regularizers l2, que sirve para La magnitud de la regularización se controla mediante el hiperparámetro sr, que puede ser ajustado durante el ajuste de hiperparámetros para optimizar el rendimiento del modelo.

Después, se contrunye el algoritmo y se utiliza el mismo que se definió anteriormente.

Luego se compila el modelo con las funciones keras.losses.MeanSquaredError y keras.metrics.MeanAbsoluteError para definir las metricas y poder conocer el desempeño del modelo.

Por último se hace un condicional donde se decide que algoritmo de optimización utilizar para compilar el modelo de la red neuronal, estas opciones pueden ser  ADAM o SGD. y retorna el modelo compilado.
"""

def model_tuning(hp):

    dor = hp.Float("DOR", min_value=0.05, max_value=0.4, step=0.05)
    sr = hp.Float("SR", min_value=0.005, max_value=0.02, step=0.003)
    fa = hp.Choice('FA_CO', ['tanh', 'sigmoid', 'relu'])
    opt = hp.Choice('opt', ['Adam', 'SGD'])

    # Definir la regularización L2 con el factor de escala ajustado por el hiperparámetro sr
    l2_r = keras.regularizers.l2(sr * 0.1)  # Ajusta manualmente el factor de escala según sea necesario

    # Construir el modelo
    ann3=keras.models.Sequential([
    keras.layers.InputLayer(input_shape=X_tr.shape[1:]),
    keras.layers.Dense(units=512, activation='relu'),
    keras.layers.Dense(units=256, activation='relu'),
    keras.layers.Dense(units=128, activation='relu'),
    keras.layers.Dense(units=64, activation='relu'),
    keras.layers.Dense(units=1, activation='relu')
])


    # Compilar el modelo
    l = keras.losses.MeanSquaredError()  # Función de pérdida para regresión
    m = keras.metrics.MeanAbsoluteError(name = 'RMSE')  # Métrica para regresión

    if opt == 'Adam':
        opt1 = keras.optimizers.Adam(learning_rate=0.01)
    else:
        opt1 = keras.optimizers.SGD(learning_rate=0.01)

    ann3.compile(optimizer=opt1, loss=l, metrics=[m])

    return ann3

"""Grilla de hiperparámetro. Se deja en el número de intentos en 5 por temas de tiempo. El número de epoch 15, y se afina con el model_tuning que es el hiperparámetro entrenado anteriormente."""

search_model=kt.RandomSearch(
    hypermodel = model_tuning, ## nombre de funcion de construccion modelo
    hyperparameters = hp,
    objective = kt.Objective('val_RMSE', direction="min"),
    max_trials = 5,
    overwrite = True,
    project_name = "res_afin"
)

"""Se busca el mejor modelo"""

search_model.search(X_tr, y_tr, epochs=15, validation_data=(X_te, y_te))
search_model.results_summary()


model_winner=search_model.get_best_models(1)[0] ### me muestra 1 modelo y escoge posicion 0

model_winner.build()
model_winner.summary()

"""Estos son los mejores hiperparámetros para el modelo teniendo en cuenta que dio como resultado un RMSE de 230."""

hps_winner = search_model.get_best_hyperparameters(1)[0]
best_hps = hps_winner.values
best_hps

model_winner.build()
model_winner.summary()

"""# 2.Exportación de los datos

* Objetos para análisis del desempeño en entrenamiento y evaluación:

y_total: Array de la variable dependiente utilizada para el entrenamiento del modelo.

hps_winner: Mejores hiperparámetros obtenidos durante el ajuste del modelo.

* Objetos para despliegue y hacer predicciones en base nueva:

sc: Objeto StandardScaler utilizado para escalar los datos durante el entrenamiento del modelo. Se exportó como "escalador".

model: Modelo de red neuronal entrenado. Se exportó como "modelo.h5".
"""

##########
import joblib

y_total=np.array(y) ### para convertir y en array
X_sc.shape

model=model_tuning(hps_winner)
model.fit(X_sc, y_total, epochs=20) ##  queda entrenado el modelo con todos los datos

#### se exportan insumos necesarios para prediccion

###escalador
nombre="escalador"
ruta="Descargas\\"

nombre_completo= ruta+nombre


joblib.dump(sc, nombre_completo) ### para exporttar objetos de python

model.save('Descargas\\modelo.h5') ## para exportar modelo utilizando la función de tensorflow para evitar conflictos