# -*- coding: utf-8 -*-
"""Trabajo _DL_Parte_1_Equipo4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tJzAe7rWHpHtgWpBtVwQjsS3IRLAyHYt

#Trabajo DL Parte 1

##Equipo 4

Integrantes:

* Simon Marin
* Santiago Cordoba
* Miguel Vargas
"""

!pip install keras-tuner

###basicas
import pandas as pd
import numpy as np

##### datos y modelos sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

####### redes neuronales

import tensorflow as tf
from tensorflow import keras


import keras_tuner as kt ### paquete para afinamiento
####instalar paquete !pip install keras-tuner


import joblib

### cargamos los datos
url='https://raw.githubusercontent.com/juancamiloespana/LEA2/master/_data/credit_card_clients.csv'
Credit_card_df= pd.read_csv(url)

"""CUST_ID: Identificación del titular de la tarjeta de crédito

BALANCE: Saldo disponible para compras

BALANCE_FREQUENCY: Frecuencia de actualización del saldo (donde 1 = actualización frecuente y 0 = no se actualiza con frecuencia).

PURCHASES: Importe de las compras realizadas por el cliente

ONEOFF_PURCHASES: Importe máximo de la compra realizada a una couta

INSTALLMENTS_PURCHASES: Importe de las compras realizadas a plazos

CASH_ADVANCE: Avances de Efectivo realizados por el usuario

PURCHASES_FREQUENCY: Frecuencia con la que se realizan las compras (1 = compras frecuentes, 0 = compras poco frecuentes)

ONEOFF_PURCHASES_FREQUENCY: frecuencia con la que se realizan las compras a una couta (1 = compras frecuentes, 0 = compras poco frecuentes)

PURCHASES_INSTALLMENTS_FREQUENCY: Frecuencia con la que se realizan compras a plazos (1 = se realizan con frecuencia, 0 = no se realizan con frecuencia)

CASH_ADVANCE_FREQUENCY: Frecuencia con la que se realizan avances de efectivo

CASH_ADVANCE_TRX: Número de transacciones realizadas por "avances de efectivo"

PURCHASES_TRX: Número de transacciones de compra realizadas

CREDIT_LIMIT: Límite de la tarjeta de crédito del usuario

PAYMENTS: Importe de los pagos realizados por el usuario

MINIMUM_PAYMENTS: Importe mínimo de los pagos realizados por el usuario

PRC_FULL_PAYMENT: Porcentaje del pago total abonado por el usuario

TENURE: Tenencia del servicio de tarjeta de crédito para el usuario
"""

## verificar nulos
Credit_card_df.info()

Credit_card_df['PURCHASES'].value_counts()

null_counts = Credit_card_df.isnull().sum()

print(null_counts)

Credit_card_df= Credit_card_df.dropna()

null_counts = Credit_card_df.isnull().sum()

print(null_counts)

### separar respuesta y explicativas
y = Credit_card_df['PURCHASES']

X = Credit_card_df.drop(['PURCHASES', 'CUST_ID'], axis=1)

##y.value_counts() ### tres categorías
### escalado de las variables
sc=StandardScaler().fit(X) ## calcular la media y desviacion para hacer el escalado
X_sc=sc.transform(X)  ## escalado cob base en variales escladas

## separar entrenamiento evaluación
X_tr, X_te, y_tr, y_te= train_test_split(X_sc, y, test_size=0.2)
X_tr.shape

"""Mod2"""

hp=kt.HyperParameters()


def hyper_mod(hp):

    dor=hp.Float("DOR", min_value=0.05, max_value=0.4, step=0.05)
    sr=hp.Float("SR", min_value=0.005, max_value=0.02, step=0.003)
    fa=hp.Choice('FA_CO', ['tanh', 'sigmoid', 'relu'])
    opt=hp.Choice('opt', ['Adam', 'SGD'])

    l2_r=keras.regularizers.l2(sr)


    ann1=keras.models.Sequential([
        keras.layers.InputLayer(input_shape=X_tr.shape[1:]),  ## capa de entrada no es necesaria
        keras.layers.Dense(units=128, activation=fa, kernel_regularizer=l2_r),
        keras.layers.Dropout(dor),
        keras.layers.Dense(units=64, activation=fa,  kernel_regularizer=l2_r),
        keras.layers.Dropout(dor),### capa oculta 1, 128 neuronas, función de activación relu
        keras.layers.Dense(units=32, activation=fa,  kernel_regularizer=l2_r),
        keras.layers.Dropout(dor),## capa oculta 2, 128 neuronas, función de activación relu
        keras.layers.Dense(units=16, activation=fa,  kernel_regularizer=l2_r),
        keras.layers.Dropout(dor),
        keras.layers.Dense(units=1)  # capa de salida , 128 neuronas, función de activación relu
    ])


    if opt == 'Adam':
        opt1= keras.optimizers.Adam(learning_rate=0.01)
    else:
        opt1= keras.optimizers.SGD(learning_rate=0.01)


    ann1.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.01),
             loss='mean_squared_error',
             metrics=['mean_absolute_error'])


    return ann1



search_model = kt.RandomSearch(
    hypermodel= hyper_mod,
    hyperparameters=hp,
    objective=kt.Objective('val_mean_absolute_error', direction='max'),
    max_trials=5,
    overwrite=True,
    directory="res",
    project_name="afin"

)

"""Aquí definimos una función de búsqueda aleatoria de hiperparametros para nuestro modelo de red neuronal. Aplicamos regularización L2 y definimos el error absoluto medio como métrica de desempeño."""

### esto es equivalente al fit del modelo
search_model.search(X_tr, y_tr, epochs=10, validation_data=(X_te, y_te))

"""este resultado nos indica que en la quinta iteración del entrenamiento, estamos obteniendo una variación entre las predicciones del modelo de 1014 y este corresponde al mejor valor obtenido durante la fase de entrenamiento del modelo"""

search_model.results_summary() ### resultados de afinamiento

model_winner=search_model.get_best_models(1)[0]

model_winner.count_params() ### una red neuronal
model_winner.summary() ### una red neuronal

hps_winner= search_model.get_best_hyperparameters(2)[1]
hps_winner.values

"""El modelo final alcanzó un MAE de 1008.3375 en el conjunto de validación, lo que significa que las predicciones del modelo tienen, en promedio, una desviación de aproximadamente 1008 unidades monetarias del valor real de las compras de los clientes en los próximos seis meses.

Este modelo puede ser extremadamente útil para predecir el comportamiento de compra de nuestros clientes y ajustar nuestras estrategias de fidelización y marketing en consecuencia. Algunas aplicaciones específicas incluyen:

Gestión de Riesgo Crediticio:

Ajustar los límites de crédito de manera proactiva para minimizar riesgos de incumplimiento y asegurar una gestión responsable del crédito.
Estrategias de Marketing Personalizado:

Identificar a los clientes que probablemente aumenten sus compras y dirigir campañas de marketing específicas, como descuentos personalizados y promociones especiales.
Optimización de Recursos:

Optimizar recursos, como personal de atención al cliente y servicios de soporte, para atender mejor a los clientes de alto valor, mejorando la eficiencia y la satisfacción del cliente.
Planes de Pago Personalizados:

Diseñar planes de pago personalizados con condiciones más favorables para los clientes con altas probabilidades de aumentar sus compras, incluyendo opciones de financiamiento y tasas de interés más bajas.
Conclusión:
El proceso de afinamiento de hiperparámetros y la evaluación del modelo han demostrado que es posible construir una red neuronal que prediga las compras futuras de los clientes con una precisión razonable. Este modelo puede ser una herramienta valiosa para mejorar la toma de decisiones y optimizar nuestras estrategias de negocio.
"""

from sklearn.metrics import mean_absolute_error

pred_test = model_winner.predict(X_te)  # Predicciones para datos de prueba
mae_test = mean_absolute_error(y_te, pred_test)
print(f"MAE en datos de prueba: {mae_test}")

pred_tr = model_winner.predict(X_tr)  # Predicciones para datos de entrenamiento
mae_train = mean_absolute_error(y_tr, pred_tr)
print(f"MAE en datos de entrenamiento: {mae_train}")

################### analizar el modelo #####

#pred_test=np.argmax(model_winner.predict(X_te), axis=1)
#X_te.shape
#print(metrics.classification_report(y_te, pred_test))


#pred_tr=np.argmax(model_winner.predict(X_tr), axis=1)
#print(metrics.classification_report(y_tr,pred_tr ))


#cm=metrics.confusion_matrix(y_te, pred_test)
#cm_disp=metrics.ConfusionMatrixDisplay(cm)
#cm_disp.plot()

##########

y_total=np.array(y) ### para convertir y en array
X_sc.shape

model=hyper_mod(hps_winner)
model.fit(X_sc, y_total, epochs=20) ##  queda entrenado el modelo con todos los datos

"""El proceso iterativo entre epoch se identifica que el mejor desempeño del modelo entrenado se da en la iteración 11 con una métrica de error medio absoluto de 928.2269 lo que nos permite realizar predicciones sobre las compras con una desviación aproximada de 928 unidades monetarias por encima o por debajo"""

model.save('salidas\\modelo.h5') ## para exportar modelo utilizando

import numpy as np
import joblib

# Definir x_train como los datos de entrada escalados
x_train = X_sc

##Nombre variables
column_names = X.columns.tolist()

##Objeto de escalado
scaler = StandardScaler().fit(X)

from sklearn.metrics import mean_absolute_error, mean_squared_error

##Tomar Metricaz de evaluación de desempeño

pred_train = model.predict(X_sc)
pred_test = model.predict(X_te)

mae_train = mean_absolute_error(y_total, pred_train)
mae_test = mean_absolute_error(y_te, pred_test)

mse_train = mean_squared_error(y_total, pred_train)
mse_test = mean_squared_error(y_te, pred_test)

evaluation_metrics = {
    'MAE_train': mae_train,
    'MAE_test': mae_test,
    'MSE_train': mse_train,
    'MSE_test': mse_test
}

### Objetos para Análisis de Desempeño en Entrenamiento y Evaluación

#'X_tr': Array con datos de entrada con los que se entrenó el modelo.
joblib.dump(X_tr, 'X_tr.pkl')

#'X_te': Array con datos de entrada para evaluación.
joblib.dump(X_te, 'X_te.pkl')

#'ann1': Modelo de red neuronal antes de la búsqueda de hiperparámetros.
joblib.dump(ann1, 'ann1.pkl')

#'hps_winner': Exportar Mejores hiperparametros.
joblib.dump(hps_winner, 'hps_winner.pkl')

#'model_winner': Mejor modelo obtenido después de la búsqueda de hiperparámetros.
joblib.dump(model_winner, 'model_winner.pkl')

# 'x_train':Datos de Entrenamiento Escalados
joblib.dump(x_train, 'x_train.pkl')

# 'evaluation_metrics':Metricaz de evalución del desempeño del modelo
joblib.dump(evaluation_metrics, 'evaluation_metrics.pkl')



### Objetos para Despliegue y Predicciones con Nuevos Datos

# 'sc': Objeto StandardScaler utilizado para escalar los datos.
joblib.dump(sc, 'scaler.pkl')

# 'model_winner: Modelo de red neuronal guardado y cargado para predicciones.
joblib.dump(model_winner, 'model_winner.pkl')

# 'column_names: Lista de nombre de las variables utilizadas en el entrenamiento del modelo
joblib.dump(column_names, 'Var_sel.pkl')

# Guardar los pesos del modelo para realizar predicciones
model.save_weights('model_weights.h5')